---
title: "CUNY SPS DATA 621 - CTG5 - HW1"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Koh"
date: "February 27, 2019"
output:
    pdf_document:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 6
        fig_height: 6
        fig_caption: true
        highlight: haddock
        df_print: kable

        #css: ./reports.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)

set.seed(123)
chooseCRANmirror(graphics=FALSE, ind=1) #I need this line in order for the file to knit on my work PC
if (!require('caret')) (install.packages('caret'))
if (!require('corrplot')) (install.packages('corrplot'))
if (!require('data.table')) (install.packages('data.table'))
if (!require('DataExplorer')) (install.packages('DataExplorer'))
if (!require('ggcorrplot')) (install.packages('ggcorrplot'))
if (!require('gridExtra')) (install.packages('gridExtra'))
if (!require('kableExtra')) (install.packages('kableExtra'))
if (!require('leaps')) (install.packages('leaps'))
if (!require('MASS')) (install.packages('MASS'))
if (!require('psych')) (install.packages('psych'))
if (!require('reshape')) (install.packages('reshape'))
if (!require('tidyverse')) (install.packages('tidyverse'))
```

```{r include=FALSE}
# load data
train <- read.csv('https://raw.githubusercontent.com/silverrainb/data621proj1/master/moneyball-training-data.csv',
                     stringsAsFactors = F, header = T)
test <- read.csv('https://raw.githubusercontent.com/silverrainb/data621proj1/master/moneyball-evaluation-data.csv',
                     stringsAsFactors = F, header = T)
# check data
str(train)
str(test)

# remove index
train$INDEX <- NULL
test$INDEX <- NULL

# clean the variable names so it is easier to use 
cleanVar <- function(data) {
    name.list <- names(data)
    name.list <- gsub("TEAM_", "", name.list)
    names(data) <- name.list
    data
}

# apply the function
train <- cleanVar(train)
test <- cleanVar(test)

# check data once again
str(train)
str(test)
```

\newpage

***

# DATA EXPLORATION

Professionals and gamblers alike are always seeking to optimize their chances of winning, whether it be sports, games, or their bets on them. Major League Baseball is a [multibillion dollar industry](https://www.forbes.com/sites/mikeozanian/2018/04/11/baseball-team-values-2018/#4675cfd43fc0) where individual teams, players, and those who profit off of their success stand to benefit most from such optimization. 

Data from 1871 to 2006 was collected in order to infer how many wins could be expected from the 162 games in a baseball team's season.  Each observation represents a season for an unnamed team, and we have a total of 2,276 observations. For each team the target variable, `TARGET_WINS`, represents the number of wins in a given year and has a maximum value of 162 possible wins.  In addition to that 15 continuous integer predictor variables were collected (not including the index) representing each team's: base hits, doubles, triples, homeruns, walks, and strikeouts by batters, batters hit by pitches, bases stolen by batters and the number of times they were caught stealing, the number of errors, double plays, walks, hits, and homeruns allowed, and strikeouts by pitchers. The testing data contains the same 15 predictor variables and no target variable so it will be impossible to check the accuracy of our predictions from the testing data.  

VARIABLE NAME  |  DEFINITION  |  THEORETICAL EFFECT ON WINS
-----------  |  --------------------  |  ----------
TARGET_WINS  |  Number of wins  |  outcome variable
BATTING_H  |  Base Hits by batters (1B,2B,3B,HR)   |  Positive Impact 
BATTING_2B  |  Doubles by batters (2B)  |  Positive Impact 
BATTING_3B  |  Triples by batters (3B)  |  Positive Impact 
BATTING_HR  |  Homeruns by batters (4B)  |  Positive Impact 
BATTING_BB  |  Walks by batters  |  Positive Impact 
BATTING_HBP  |  Batters hit by pitch (get a free base)  |  Positive Impact 
BATTING_SO  |  Strikeouts by batters  |  Negative Impact 
BASERUN_SB  |  Stolen bases  |  Positive Impact 
BASERUN_CS  |  Caught stealing  |  Negative Impact 
FIELDING_E  |  Errors  |  Negative Impact 
FIELDING_DP  |  Double Plays  |  Positive Impact 
PITCHING_BB  |  Walks allowed  |  Negative Impact 
PITCHING_H  |  Hits allowed  |  Negative Impact 
PITCHING_HR  |  Homeruns allowed  |  Negative Impact 
PITCHING_SO  |  Strikeouts by pitchers  |  Positive Impact 

\vspace{12pt}

## Summary Statistics

```{r}
describe(train)[,c(2,8,3,5,9,4)]
```

Looking at the above, it can be easily noted that there are outliers present in more than variable, with `PITCHING_H` being the worst offender. Even at three times the standard deviation, its maximum value lays far outiside of the 68-95-99.7 rule. `FIELDING_E`, on the other hand, has the curious case of having a large difference between its mean and median, indicating there is skew present in this variable as well before any charts are actively looked at. Skewed variables cause bias in linear models and need treatment before being used.

\vspace{12pt}

## Shape of Predictor Distributions

The distribution of most of the variables seems normal although `BASERUN_SB`, `BASERUN_CS`, and `BATTING_3B` have a slight to moderate right skew, `FIELDING_E`, `PITCHING_BB`, `PITCHING_H`, and `PITCHING_SO` have an extreme right skew, and `BATTING_HR`, `BATTING_SO`, and `PITCHING_HR` are bimodal.  As a result some data transformation will most likely be necessary to improve the accuracy of our model.  The standard deviation of the various variables also hints at the intense skewing of some of the variables.

```{r fig.height=4, fig.cap="Data Distributions"}
Hist <- train %>%
    gather() %>%
    ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram(fill = "#58BFFF") +
    xlab("") +
    ylab("") +
    #ggtitle("Data Distributions") +
    theme(panel.background = element_blank())
Hist
```

## Outliers

There are also a large number of outliers that need to be accounted for, most prevalently in `FIELDING_E` and `BATTING_H` based off of the boxplots above. One such extreme outlier removed implied that there were, on average per game in a single season, 186 hits allowed by pitchers. This is an unrealistic figure, even for those for whom baseball is outside of their realm of understanding.

\vspace{12pt}

```{r fig.height=4, fig.cap="Boxplots highlighting many outliers in the data."}
melt.train <- melt(train)
# Boxplot
ggplot(melt.train, aes(variable, value)) + 
    geom_boxplot(width=.5, fill="#58BFFF", outlier.colour="#58BFFF", outlier.size = 1) +
    scale_y_log10() + 
    stat_summary(aes(colour="mean"), fun.y=mean, geom="point",
                 size=2, show.legend=TRUE) +
    stat_summary(aes(colour="median"), fun.y=median, geom="point", 
                 size=2, show.legend=TRUE) +
    coord_flip(ylim = c(0, 2200), expand = TRUE) +   
    scale_y_continuous(labels = scales::comma,
                       breaks = seq(0, 2200, by = 200)) + 
    labs(colour="Statistics", x="", y="log transformed freq.") + 
    scale_colour_manual(values=c("red", "blue")) +
    theme(panel.background=element_blank(), legend.position="top")
```

\vspace{12pt}

## Missing Values

Of all the observations gathered across these fifteen variables, there are 3,478 missing values out of 36,416 total data points, which represents 10.187% of the data.  Batters hit by pitches was missing the most, with 2,085 instances of missing information, which represents 91.61% of that variable missing. Additionally `Pitching_SO` and `Batting_SO` are missing exact same proportion 4.48% and are missing in the same observations.  This data may not be missing at random and so there may be cause for removing it.

\vspace{12pt}

```{r fig.height=4, fig.cap="Missing values"}
# Missing values
#table(is.na(train)) #3478 missing values
#sapply(train, function(x) sum(is.na(x)))
plot_missing(train)  
```

\vspace{12pt}

## Linearity

Each variable was tested against the target variable in order to determine at a glance which had the most potential linearity before the dataset was modified.

\vspace{12pt}

```{r, fig.cap="Linear relationships between each predictors and the target"}
train %>%
  gather(-TARGET_WINS, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = TARGET_WINS)) +
    geom_point(alpha=0.1) +
    stat_smooth() +
    facet_wrap(~ var, scales = "free", ncol=3) +
    ylab("TARGET_WINS") +
    xlab("") +
    #ggtitle("Each Predictor vs. Target") + 
    theme(panel.background = element_blank())
```

As can be observed, the most influential variables are the ones previously discussed to have severe outliers and skew, and their linear relationship is negative - the higher the variable, the lower the target wins. On the other hand, `BATTING_H`, `BATTING_BB` and `BATTING_2B` showed the most promise. 

\newpage

***

# DATA PREPARATION

## Missing Values

As previously mentioned, just north of 10% of the data was missing values. Missing values can lead to errors in a model, bias, and worse if left unaccounted for. Attempting to "fix" this by imputing values or guessing why the values are missing in the first place - such as concluding that the missing values are meant to be zeroes - are just as likely to help with creating a model as it is to help with creating a disaster.

One of the R packages utilized, DataExplorer, which was used for the chart above, recommends removing null or missing values above a certain threshold as indicated in the graph. 

Fixing missing values with imputation may help, but can also have a negative impact on the model if the assumed values do not correspond to the actual missing values.  When it is just a few observations missing, modifications can be made, however, 91.61% is too large a proportion and would almost definitely distort the model, so we decided it was better to remove the `BATTING_HBP` column altogether. Deleting all cases with missing values, in this instance, would have shrunk the size of the dataset down to less than a tenth of its original size. If we simply delete all cases with missing values from the analysis, we will cause no bias, but we would most certainly lose a lot of important information.

Data that is Missing Completely at Random (MCAR), meaning the probability that a value is missing is the same for all cases can be imputed. Although there is some concern about whetherr or not `Pitching_SO` and `Batting_SO` are MCAR, we chose to leave all the remaining variables except `BATTING_HBP` and determine whether or not to remove themn during the modelling process.  

\vspace{12pt}

```{r}
# Train.Mod being made here
# Fix missing values
# remove BATTING_HBP
train.mod <- subset(train, select = -c(BATTING_HBP)) # removes variable with over 90% missing values
```

\vspace{12pt}

### NA Imputation

To deal with the remaining missing values, the bag imputation method was used via the caret package. A set of dummy variables were created and were used to predict the various values in the dataset. This dummy-set was then pre-processed and used against itself to predict the missing values.

```{r}
train.mod <- as.data.table(train.mod)
dummies <- dummyVars(~ ., data = train.mod[, -1])
train.dummy <- predict(dummies, train.mod)
pre.process <- preProcess(train.dummy, method='bagImpute')
imputation <- as.data.frame(predict(pre.process, train.dummy))

imputed_train <- cbind(train.mod$TARGET_WINS, imputation)
names(imputed_train)[1] <- "TARGET_WINS"
```


```{r fig.cap="Data before imputing values"}
train.mod.desc <- describe(train.mod)[,c(2,8,3,5,9,4)]
train.mod.desc
```


```{rfig.cap="Data after imputing values"}
imputed_train.desc <- describe(imputed_train)[,c(2,8,3,5,9,4)] # for comparison...
imputed_train.desc
```


```{rfig.cap="Difference between original and imputed data"}
train.mod.desc - imputed_train.desc # show the difference between our original data and the imputed data...  Maybe only need this one?
```

\vspace{12pt}

## Remove Outliers

Outlier treament was done by placing a threshold of five times the standard deviation up from the mean and removing all observations that fell north of this boundary.

\vspace{12pt}

```{r}
max_sd = 5 # change this number to change the threshold for how many standard deviations from the mean are acceptable

outliers <- sapply(imputed_train[,-1], function(x) ifelse(x < mean(x)+(sd(x)*max_sd), TRUE, NA))
#outliers <- sapply(imputed_train[,-1], function(x) ifelse(findInterval(x, c(mean(x)-(sd(x)*max_sd),mean(x)+(sd(x)*max_sd)), rightmost.closed = T) == 1, TRUE, NA))
imputed_train <- imputed_train[complete.cases(outliers),]
```

\vspace{12pt}

```{r}
#Why did we do this?  Aren't both train.mod and imputed_train the same now?
sapply(train.mod, function(x) sum(is.na(x)))

train.mod[, `:=`(BATTING_SO = imputation$BATTING_SO,
          BASERUN_SB = imputation$BASERUN_SB,
          BASERUN_CS = imputation$BASERUN_CS,
          PITCHING_SO = imputation$PITCHING_SO,
          FIELDING_DP = imputation$FIELDING_DP)]
```

\vspace{12pt}

## Correlation

The theoretical effect of strikeouts by batters, batters caught stealing, errors, walks, hits, and homeruns allowed were believed to have a negative impact on the number of wins of an individual team in a given year. A closer look at the correlation plot between the variables painted a different picture.

When compared to what was hypothesized, there was actually a positive impact for the number of wins for a team in a given year by walks, hits, and homeruns allowed; at the same time, variables previously thought to have a positive correlation - strikeouts by pitchers and double plays - had a negative correlation for the number of wins. The three variables with the greatest correlation to the number of wins were the hits allowed, the walks by batters, and the walks allowed. Of these, the hits allowed had a relatively low correlation with the walks by batters and the walks allowed, whereas the walks allowed and the walks by batters had a direct positive correlation with one another.

\vspace{12pt}

```{r}
# Correlations
corr.train <- round(cor(imputed_train),3)
ggcorrplot::ggcorrplot(corr.train, 
                       type = 'lower',
                       lab=T,
                       lab_size=2,
                       title="Correlation")
```

\vspace{12pt}

***

## Feature Engineering

Jeremy: Adjusted this to reflect offense (batting) minus defense (pitching).  These arithmetically transformed offense / defense variables are linearly related with BATTING and PITCHING variables, so we can include one or the other in a model, but not both.  Replacing original variables with these transforms did not improve $R^2$ in a base case.

```{r}
imputed_train$BP_H <- imputed_train$BATTING_H - imputed_train$PITCHING_H
imputed_train$BP_HR <- imputed_train$BATTING_HR - imputed_train$PITCHING_HR
imputed_train$BP_BB <- imputed_train$BATTING_BB - imputed_train$PITCHING_BB
imputed_train$BP_SO <- imputed_train$BATTING_SO - imputed_train$PITCHING_SO
```

----

FOR THE OTHER HALF OF THE GROUP:

z_train <- sapply(imputed_train, scale)
log_train <- log(imputed_train) # weird results
z_log_train <- sapply(log_train, scale) # weirder results

imputed_train is most likely the variable you want to use.

\newpage

***

# BUILD MODELS

## Instructions:

Using the training data set, build at least three different multiple linear regression models, using different variables (or the same variables with different transformations). Since we have not yet covered automated variable selection methods, you should select the variables manually (unless you previously learned Forward or Stepwise selection, etc.). Since you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done.
Discuss the coefficients in the models, do they make sense? For example, if a team hits a lot of Home Runs, it would be reasonably expected that such a team would win more games. However, if the coefficient is negative (suggesting that the team would lose more games), then that needs to be discussed. Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.

\vspace{12pt}

***

## MODEL 1

Multiple regression can be created as a purely statistical model, through the use of significance tests, or it can be interpreted in a more practical, non-statistical manner. This approach is based on the subject-area expertise.

We've created the following categories from the most important to the least important variables according to the subject-area expert.

Very Important: 
BATTING_H, BATTING_HR, BATTING_SO ,FIELDING_E, PITCHING_SO

Fairly Important:
BASERUN_SB, PITCHING_HR, BATTING_BB

Important:
BATTING_2B, BATTING_3B, FIELDING_DP, PITCHING_H

Slightly Important:
PITCHING_BB, BASERUN_CS

Not at all important:
BATTING_HBP


'Batters hit by pitch' and 'Caught Stealing' have been eliminated as least important variables according to the expert. 

```{r}

model_exp <- lm(TARGET_WINS ~  BATTING_H + BATTING_HR + BATTING_SO + FIELDING_E + 
    PITCHING_SO + BASERUN_SB + PITCHING_HR + BATTING_BB + BATTING_2B + 
    BATTING_3B + FIELDING_DP + PITCHING_BB + PITCHING_H ,
data = imputed_train)
summary(model_exp)
```

\vspace{12pt}

We got 0.2793 on Adjusted R-squared after we removed these two variables. Once we tried to remove other not very important variables according to subject-area expert, we got an even lower R-squared.

The next step we performed was backward elimunation, which was more effective compared to forward selection. BATTING_H and BATTING_2B have been removed based on the Backward Selection results.

\vspace{12pt}

```{r}
step(model_exp, direction = "backward")
```

```{r}
model_exp2 <- lm(TARGET_WINS ~   BATTING_HR + BATTING_SO + FIELDING_E + 
    PITCHING_SO + BASERUN_SB + PITCHING_HR + BATTING_BB  + 
    BATTING_3B + FIELDING_DP + PITCHING_BB + PITCHING_H ,
data = imputed_train)

summary(model_exp2)
```

\vspace{12pt}

Our R-squared was still low (0.276), so we decided to look at the outliers, which can affect our model. Pitching_h had the high number of outliers which indicated a need for data transformation. We decided to use log transformation for this variable.

\vspace{12pt}

```{r}
stripchart(data.frame(scale(imputed_train)), method ="jitter", las=2,
vertical=TRUE)
```

```{r}
model_exp3 <- lm(TARGET_WINS ~   BATTING_HR + BATTING_SO + FIELDING_E + 
    PITCHING_SO + BASERUN_SB + PITCHING_HR + BATTING_BB  + 
    BATTING_3B + FIELDING_DP + PITCHING_BB + log(PITCHING_H) ,
data = imputed_train)
summary(model_exp3)
```

\vspace{12pt}

After we used the log trasformation our model's Adjusted R-squared increased to 0.2921.

\vspace{12pt}

```{r fig.height=3, fig.cap="Model 1: Residual Plot and Q-Q Plot"}
#par(mfrow=c(2,1))
residplot_exp <- ggplot(data = model_exp3, 
                               aes(x = .fitted, 
                                   y = .resid)) +
  geom_point(aes(y = .resid, 
                 color = .resid)) +
  scale_color_gradient2(low = "midnightblue", 
                        mid = 'white', 
                        high = 'red2') +
  stat_smooth(method = 'loess', 
              se = TRUE, 
              fill = 'gray95', 
              color = 'darkgray') +
  geom_hline(yintercept = 0, 
             col = "black", 
             linetype = "dashed", 
             alpha = .8, 
             size = .5) +
  guides(color = FALSE) +
  labs(x = 'Fitted Values', 
       y = 'Residuals') +
  theme_minimal() +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  theme(plot.title = element_text(hjust = .5))

qqplot_exp <- ggplot(data = imputed_train, aes(sample = TARGET_WINS)) +
  stat_qq(size = 1.5) +
  stat_qq_line(color = 'darkgray') +
  labs(x = "Theoretical Quantiles", 
       y = "Standardized Residuals") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = .5))

grid.arrange(residplot_exp, qqplot_exp, nrow = 1)
```

```{r}
pred_exp <- predict(model_exp3, test) 
summary(pred_exp)
```

```{r}
broom::glance(model_exp3)
```

\vspace{12pt}

Summary of Results for Model 1:

The overall Subject-Area expertise wasn't as effective as a stand alone method of creating multiple regression models. Statistical iterations which were performed contradicted the subject area expert, such as, removing Batting_H from the model. Additionally log tranformation of PITCHING_H made a significant improvment in our model linearity.

\vspace{12pt}

***

## MODEL 2

Our approach for Model 2 was to try to use as many of the tools as possible that are available in R and that we have learned thus far to determine a model based solely on the statistical qualities of the predictor variables without any regard to our expert's opinion.

We started by plotting the relationships between variables that had high correlation values to look for potential collinearity problems. 

\vspace{12pt}

```{r fig.height=4, fig.cap="Scatterplots showing possible collinearity problems"}

plot1 <- ggplot(imputed_train, aes(x = BATTING_HR, y = PITCHING_HR)) +
    geom_point(alpha=0.1) +
    stat_smooth() +
    theme(panel.background = element_blank())

plot2 <- ggplot(imputed_train, aes(x = BATTING_HR, y = BATTING_SO)) +
    geom_point(alpha=0.1) +
    stat_smooth() +
    theme(panel.background = element_blank())

plot3 <- ggplot(imputed_train, aes(x = BATTING_BB, y = PITCHING_BB)) +
    geom_point(alpha=0.1) +
    stat_smooth() +
    theme(panel.background = element_blank())

plot4 <- ggplot(imputed_train, aes(x = BATTING_SO, y = PITCHING_SO)) +
    geom_point(alpha=0.1) +
    stat_smooth() +
    theme(panel.background = element_blank())

plot5 <- ggplot(imputed_train, aes(x = BATTING_SO, y = PITCHING_HR)) +
    geom_point(alpha=0.1) +
    stat_smooth() +
    theme(panel.background = element_blank())

plot6 <- ggplot(imputed_train, aes(x = BASERUN_SB, y = BASERUN_CS)) +
    geom_point(alpha=0.1) +
    stat_smooth() +
    theme(panel.background = element_blank())

grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, nrow = 2)
```

\vspace{12pt}

Based on the charts above we decided somewhat arbitrarily to remove the three pitching variables (`PITCHING_HR`, `PITCHING_BB`, and `PITCHING_SO`) rather than the corresponding batting variables (`BATTING_HR`,  `BATTING_BB`, and `BATTING_SO`) due to the extremely high correlation between these predictors.  We then plotted the reamining variabes to see if they showed a linear relationship with the target variable.  Most of the premaining predictors showed a clear linear relationship with the target, however, the extreme skew of `PITCHING_H` and `FIELDING_E` as well as a more moderate skew in `BASERUN_SB` and `BATTING_3B`, can be seen in the plots.

\vspace{12pt}

```{r, fig.cap = "Linear relationship between each predictor and the arget"}
lm_data <- imputed_train[,-c(9,11:13,16:19)]
lm_data %>%
  gather(-TARGET_WINS, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = TARGET_WINS)) +
    geom_point(alpha=0.1) +
    stat_smooth() +
    facet_wrap(~ var, scales = "free", ncol=4) +
    xlab("") +
    ylab("TARGET_WINS") +
    theme(panel.background = element_blank())
```

\vspace{12pt}

```{r fig.height=4, fig.cap="Predictor variable distributions"}
Histograms <- lm_data %>%
    gather() %>%
    ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram(fill = "#58BFFF") +
    xlab("") +
    ylab("") +
    #ggtitle("Histograms") +
    theme(panel.background = element_blank())
Histograms
```

\vspace{12pt}


### Log Transform Data

We decided to log transform `PITCHING_H`, `FIELDING_E`, `BASERUN_SB` and `BATTING_3B` in order to compensate for the skew.  The resulting distributions can be seen in the revised plots below.

```{r}
to_log <- c("BASERUN_SB", "BATTING_3B", "FIELDING_E", "PITCHING_H")
log_lm_data <- lm_data
log_lm_data[,to_log] <- log(log_lm_data[,to_log])
```

\vspace{12pt}

```{r, fig.height=2, fig.cap = "Linear relationship between each log transformed predictor and the Target showing decreased skew"}
log_lm_data[,c(to_log, "TARGET_WINS")] %>%
  gather(-TARGET_WINS, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = TARGET_WINS)) +
    geom_point(alpha=0.1) +
    stat_smooth() +
    facet_wrap(~ var, scales = "free", ncol=4) +
    xlab("") +
    ylab("TARGET_WINS") +
    theme(panel.background = element_blank())
```

\vspace{12pt}

```{r fig.height=2, fig.cap="Log transformed distributions showing decreased skew"}
Histograms <- log_lm_data[,to_log] %>%
    gather() %>%
    ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free", ncol=4) +
    geom_histogram(fill = "#58BFFF") +
    xlab("") +
    ylab("") +
    ggtitle("Histograms") +
    theme(panel.background = element_blank())
Histograms
```
\vspace{12pt}

### Building the Model

Finally we built a model based on the selected variables including the log transformations where appropriate.  

```
TARGET_WINS ~ BATTING_H + BATTING_2B + log(BATTING_3B) + BATTING_HR + 
    BATTING_BB + BATTING_SO + log(BASERUN_SB) + log(PITCHING_H) + 
    log(FIELDING_E) + FIELDING_DP
```

All of the variables had a very low p-value indicating a significant impact on our target, however our $R^2$ value was low at only 0.2889.

\vspace{12pt}

```{r}
lm_data <- data.frame(lm_data)

# Basic linear model with all variables
lm1 <- lm(TARGET_WINS ~ BATTING_H + BATTING_2B + log(BATTING_3B) + BATTING_HR + BATTING_BB + BATTING_SO + log(BASERUN_SB) + log(PITCHING_H) + log(FIELDING_E) + FIELDING_DP, lm_data)
lm_summary <- summary(lm1)
```

#### $R^2$ `r lm_summary$r.squared`

```{r}
kable(lm_summary$coef, caption = "Full Model Coefficients")
```


\vspace{12pt}

```{r fig.height=3, fig.cap="Model 2: Residual Plot and Q-Q Plot"}
#par(mfrow=c(2,2))
#lm_plot <- plot(lm1)
#lm_plot

par(mfrow=c(2,1))
residplot_exp <- ggplot(data = lm1, 
                               aes(x = .fitted, 
                                   y = .resid)) +
  geom_point(aes(y = .resid, 
                 color = .resid)) +
  scale_color_gradient2(low = "midnightblue", 
                        mid = 'white', 
                        high = 'red2') +
  stat_smooth(method = 'loess', 
              se = TRUE, 
              fill = 'gray95', 
              color = 'darkgray') +
  geom_hline(yintercept = 0, 
             col = "black", 
             linetype = "dashed", 
             alpha = .8, 
             size = .5) +
  guides(color = FALSE) +
  labs(x = 'Fitted Values', 
       y = 'Residuals') +
  theme_minimal() +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  theme(plot.title = element_text(hjust = .5))

qqplot_exp <- ggplot(data = lm_data, aes(sample = TARGET_WINS)) +
  stat_qq(size = 1.5) +
  stat_qq_line(color = 'darkgray') +
  labs(x = "Theoretical Quantiles", 
       y = "Standardized Residuals") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = .5))

grid.arrange(residplot_exp, qqplot_exp, nrow = 1)
```

\vspace{12pt}

Our residuals look normally distributed and random, and with constant variability, no indication of homoscedasticity.  However we thought we may be able to use some other tools in R to refine our model and get a better $R^2$ value.  So next we tried using the leaps package to see if it would recommend removing any of our chosen variables from the model.  In the following plot you can see that we could remove `BATTING_H`, `BATTING_2B` without affecting out $R^2$ much, but it would not improve the model.  

\vspace{12pt}

```{r fig.height=10}
# All Subsets Regression from leaps package
leaps <- regsubsets(x=log_lm_data[,-1], y=log_lm_data[,1], nbest=3)
# plot a table of models showing variables in each model.
# models are ordered by the selection statistic.
leaps_plot <- plot(leaps, scale="r2")
leaps_plot
```

\vspace{12pt}

Next we tried standardizing the (non-log-transformed) variables to see what impact that might have on our model.  As you can see standardzing actually resulted in a significant reduction in our $R^2$ value from 0.2889 to 0.274.

\vspace{12pt}

```{r}
# Scale all the predictor variables
z_train <- data.frame(cbind(lm_data[,1],sapply(lm_data[,-1], scale)))
# Linear model using all scaled predictors
colnames(z_train)[1] <- "TARGET_WINS"
scaled_lm <- lm(TARGET_WINS ~ BATTING_H + BATTING_2B + BATTING_3B + BATTING_HR + BATTING_BB + BATTING_SO + BASERUN_SB + PITCHING_H + FIELDING_E + FIELDING_DP, z_train)
scaled_lm_summary <- summary(scaled_lm)
#scaled_lm_summary
```

#### $R^2$ `r scaled_lm_summary$r.squared`

```{r}
kable(scaled_lm_summary$coef, caption = "Full SCALED Model Coefficients")
```

\vspace{12pt}

```{r fig.height=3, fig.cap="Revised Model 2: Residual Plot and Q-Q Plot", eval=FALSE, include=FALSE}
par(mfrow=c(2,1))
residplot_exp <- ggplot(data = scaled_lm, 
                               aes(x = .fitted, 
                                   y = .resid)) +
  geom_point(aes(y = .resid, 
                 color = .resid)) +
  scale_color_gradient2(low = "midnightblue", 
                        mid = 'white', 
                        high = 'red2') +
  stat_smooth(method = 'loess', 
              se = TRUE, 
              fill = 'gray95', 
              color = 'darkgray') +
  geom_hline(yintercept = 0, 
             col = "black", 
             linetype = "dashed", 
             alpha = .8, 
             size = .5) +
  guides(color = FALSE) +
  labs(title = 'Standardized Model 2: Residual Plot', 
       x = 'Fitted Values', 
       y = 'Residuals') +
  theme_minimal() +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  theme(plot.title = element_text(hjust = .5))

qqplot_exp <- ggplot(data = z_train, aes(sample = TARGET_WINS)) +
  stat_qq(size = 1.5) +
  stat_qq_line(color = 'darkgray') +
  labs(title = 'Standardized Model 2: Q-Q Plot', 
       x = "Theoretical Quantiles", 
       y = "Standardized Residuals") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = .5))

grid.arrange(residplot_exp, qqplot_exp, nrow = 1)
```

```{r fig.height=10, eval=FALSE, include=FALSE}
# All Subsets Regression from leaps package on SCALED data
scaled_leaps <- regsubsets(x=z_train[,-1], y=z_train[,1], nbest=3)
# plot a table of models showing variables in each model.
# models are ordered by the selection statistic.
scaled_leaps_plot <- plot(scaled_leaps, scale="r2")
scaled_leaps_plot
```

\vspace{12pt}

### Test all of the predictors

Next we ran an ANOVA test to compare our model to the null model.  With a p-value that is basically zero, clearly our model is statistically significant.

\vspace{12pt}

```{r}
#nullmod
nullmod <- lm(TARGET_WINS ~ 1, lm_data)
anova(nullmod, lm1)
```

```{r eval=FALSE, include=FALSE}
### Test one predictor
lm2 <- lm(TARGET_WINS ~ ., lm_data[, -2])
anova(lm2, lm1)
```

```{r eval=FALSE, include=FALSE}
### Test one predictor
lm3 <- lm(TARGET_WINS ~ ., lm_data[, -3])
anova(lm3, lm1)
```

\vspace{12pt}

### Testing a subspace

We then tried testing a subspace.  Since our initial models using the difference between the corresponding batting and pitching variables did not show promise we tried adding those two variables instead. 

\vspace{12pt}

```{r}
all_data <- imputed_train[,-c(16:19)]
lm4 <- lm(TARGET_WINS ~ I(BATTING_HR+PITCHING_HR)+I(BATTING_BB+PITCHING_BB)+
              I(BATTING_SO+PITCHING_SO)+BATTING_H+BATTING_2B+log(BATTING_3B)+
              log(BASERUN_SB)+BASERUN_CS+log(PITCHING_H)+log(FIELDING_E)+FIELDING_DP, all_data)
lm4_summary <- summary(lm4)
```

#### $R^2$ `r lm4_summary$r.squared`

```{r}
kable(lm4_summary$coef, caption = "Subspace Model Coefficients")
```

\vspace{12pt}

Once again our model declined in performance rather than improving.

\vspace{12pt}

Last, but not least, we used the stepAIC function from the MASS package to see if it came up with different recommendations for what variabels to keep and which to exclude from our model.  We started with all variables putting back the ones we had previously taken out due to collinearity issues and let the algorithm choose which to keep.  

```{r include=FALSE}
mod_1 <- lm(TARGET_WINS ~ ., imputed_train)
step <- stepAIC(mod_1, direction="both")
#step$anova # display results
```

The final suggested model was:

```
Final Model:
TARGET_WINS ~ BATTING_H + BATTING_3B + BATTING_HR + BATTING_BB + 
    BATTING_SO + BASERUN_SB + BASERUN_CS + PITCHING_H + PITCHING_BB + 
    PITCHING_SO + FIELDING_E + FIELDING_DP
```

In comparison to our original model we had the following variables added to our model (`BASERUN_CS`, `PITCHING_BB`, and `PITCHING_SO`) and the following variable removed (`BATTING_2B`).

We tried multiple iterations of that model, without any log transformations, with log transformations, with and without the collinear variables, but whever we removed one of the collinear variables our model would decline in performance, so we decided to try our multiplying the corresponding collinear variables to gether and BINGO!  We got and $R^2$ of .3247 using the following model:

```
TARGET_WINS ~ BATTING_3B + BATTING_HR + BATTING_BB*PITCHING_BB + 
    BATTING_SO*PITCHING_SO + BASERUN_SB + BASERUN_CS + BATTING_H*log(PITCHING_H) + 
    log(FIELDING_E) + FIELDING_DP
```

\vspace{12pt}

```{r}
#changed to BATTING_BB*PITCHING_BB, BATTING_SO*PITCHING_SO, BATTING_H*log(PITCHING_H
mod_3 <- lm(TARGET_WINS ~ BATTING_3B + BATTING_HR + BATTING_BB*PITCHING_BB + 
    BATTING_SO*PITCHING_SO + BASERUN_SB + BASERUN_CS + BATTING_H*log(PITCHING_H) + 
    log(FIELDING_E) + FIELDING_DP, imputed_train)

mod_3_summary <- summary(mod_3)
```

#### $R^2$ `r mod_3_summary$r.squared`

```{r}
kable(mod_3_summary$coef, caption = "Full Model Coefficients")
```

\vspace{12pt}

### Predictions

We ran predictions on our final model and plotted the distribution next to the distribution from our target in the training data set to compare...

```{r}
predictions <- round(predict(mod_3, test))
```

```{r fig.height=2, fig.cap="Predictions vs. training data"}
p1 <- ggplot(data.frame(predictions), aes(predictions)) +
    geom_histogram(fill = "#58BFFF", bins = 20) +
    xlab("Test Data Predictions") +
    ylab("") +
    theme(panel.background = element_blank())

p2 <- ggplot(lm_data, aes(TARGET_WINS)) +
    geom_histogram(fill = "#58BFFF", bins = 20) +
    xlab("Training Data") +
    ylab("") +
    theme(panel.background = element_blank())

grid.arrange(p1, p2, nrow = 1)
```

***

## MODEL 3  

We sought to explore whether there was a relationship between wins and the difference of specific offensive and defensive team capabilities - hits, homeruns, balls, and strike-outs.  Incorporating variables that reflect those differences (i.e. subtracting batting hits from pitching hits, and so on), however, did not improve the explantory power of the model beyond using the original variables.

Given these variables did not yield improvements, in their place we explored a third model.  As the histograms below highlight, a number of the independent variables - pitching hits, pitching homeruns, pitching strikeouts - demonstrate pronounced rightward-skew.   

```{r fig.height=1.5, fig.cap="Histograms of variables showing pronounced rightward-skew"}

Histograms <- train.mod[,c(10, 12:14)] %>%
    gather() %>%
    ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free", nrow = 1) +
    geom_histogram(fill = "#58BFFF") +  
    theme(panel.background = element_blank())

Histograms
```

We corrected for that skew by transforming those three variables using natural logarithms.  When we tested those log transformations in a model where they replaced the untransformed, original variables combined with all other variables, we found that neither the originals nor the log transformations for pitching homeruns and pitching strikeouts met the threshold of significance (a p-value below the $\alpha$ level of .05).  Based on high p-values, over a series of backward steps we removed pitching homeruns, pitching strikeouts, and baserun caught stealing, yielding the  following model:

[Jeremy: team, should we write LaTeX formulas for each model or just cable model coefficients?]

$y = $

```{r}

logtransform_lm <- lm(TARGET_WINS ~ 
                 BATTING_H
                 + BATTING_2B
                 + BATTING_3B
                 + BATTING_HR
                 + BATTING_BB
                 + BATTING_SO
                 + BASERUN_SB
                 #+ BASERUN_CS
                 + log(PITCHING_H)
                 #+ log(PITCHING_HR + .0001) # p-value around .16 as log .27 w/o so remove
                 + PITCHING_BB
                 #+ log(PITCHING_SO + .0001) # p-value around .5 whether or not log transform
                 + FIELDING_E
                 + FIELDING_DP
                 , data = imputed_train)

logtransform_lm_summary <- summary(logtransform_lm)

#logtransform_lm_summary

kable(logtransform_lm_summary$coef, caption = "Log Transform Model Coefficients")

```

Based on this model's F-statistic and p-value, we can reject the null hypothesis that coefficients with values of zero would fit the data better.  Per the adjusted $r^2$ value, this model explains approximately $29.56\%$ of the variance in wins.  However, in doing so it treats the batting hits and batting second base runs as drags on wins (with negative coefficients), and pitching hits as buoying wins - which is counterunituitive.  While the other coefficients make more intuitive sense, these signs call into question how effectively we can use this model to understand the relationships between the independent variables and wins.

\newpage

***

# SELECT MODELS

## Instructions:

Decide on the criteria for selecting the best multiple linear regression model. Will you select a model with slightly worse performance if it makes more sense or is more parsimonious? Discuss why you selected your model.
For the multiple linear regression model, will you use a metric such as Adjusted R2, RMSE, etc.? Be sure to explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. Make predictions using the evaluation data set.

\vspace{12pt}

## Comparison of models

[Jeremy: added in a chart for model 3]

```{r fig.height=3, fig.cap="Model 3: Residual Plot and Q-Q Plot""}

# Model 3: append predictions and residuals  
imputed_train$logtransform_pred <- predict(logtransform_lm)
imputed_train$logtransform_resid <- residuals(logtransform_lm)

# Model 3: residual plot
logtransform_residplot <- ggplot(data = logtransform_lm, # for each model update chart object name and dataframe 
                               aes(x = .fitted, 
                                   y = .resid)) +
  geom_point(aes(y = .resid, 
                 color = .resid)) +
  scale_color_gradient2(low = "midnightblue", 
                        mid = 'white', 
                        high = 'red2') +
  stat_smooth(method = 'loess', 
              se = TRUE, 
              fill = 'gray95', 
              color = 'darkgray') +
  geom_hline(yintercept = 0, 
             col = "black", 
             linetype = "dashed", 
             alpha = .8, 
             size = .5) +
  guides(color = FALSE) +
  labs(x = 'Fitted Values', 
       y = 'Residuals') +
  theme_minimal() +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  theme(plot.title = element_text(hjust = .5))

# Model 3: QQ-plot residuals 
logtransform_qqplot <- ggplot(logtransform_lm, aes(sample = .stdresid)) +  # for each model update chart object name and model object 
  stat_qq(size = 1.5) +
  stat_qq_line(color = 'darkgray') +
  labs(x = "Theoretical Quantiles", 
       y = "Standardized Residuals") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = .5))

grid.arrange(logtransform_residplot, logtransform_qqplot, nrow = 1)
```

\vspace{12pt}

# Multi-collinearity

[Jeremy: We can place the correlelogram here or revisit it]

An examination of the partial correlation coefficients between all the independent variables shows that there the expected relationships between the offense and defense variables - i.e. batting and pitching homeruns - are there, meet p-value thresholds, and are strong.  This finding suggests keeping only one of each pair of variables in a given model; yet, when we tried this, less of the variance in wins was explained by the model.

```{r}

# we can move this to the beginning if we keep
library(ppcor)

collintest_df <- imputed_train[,3:15]

# subset dataframe for independent variables
collintest_stats <- pcor(collintest_df, method = 'pearson')

kable(collintest_stats$estimate)
kable(collintest_stats$p.value)
kable(collintest_stats$statistic)

```
