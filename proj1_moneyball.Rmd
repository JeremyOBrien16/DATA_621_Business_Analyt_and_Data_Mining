---
title: "CUNY SPS DATA 621 - CTG5 - HW1"
author: "Betsy Rosalen, Gabrielle Bartomeo, Jeremy O'Brien, Lidiia Tronina, Rose Koh"
date: "February 27, 2019"
output:
    pdf_document:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 8
        fig_height: 8
        fig_caption: true
        highlight: haddock
        df_print: kable

        #css: ./reports.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo=FALSE, message=FALSE, warning=FALSE)
if (!require('psych')) (install.packages('psych'))
if (!require('DataExplorer')) (install.packages('DataExplorer'))
if (!require('reshape')) (install.packages('reshape'))
if (!require('ggplot2')) (install.packages('ggplot2'))
if (!require('corrplot')) (install.packages('corrplot'))
if (!require('data.table')) (install.packages('data.table'))
if (!require('caret')) (install.packages('caret'))
```

```{r include=FALSE}
# load data
train <- read.csv('https://raw.githubusercontent.com/silverrainb/data621proj1/master/moneyball-training-data.csv',
                     stringsAsFactors = F, header = T)
test <- read.csv('https://raw.githubusercontent.com/silverrainb/data621proj1/master/moneyball-evaluation-data.csv',
                     stringsAsFactors = F, header = T)
# check data
str(train)
str(test)

# remove index
train$INDEX <- NULL
test$INDEX <- NULL

# clean the variable names so it is easier to use 
cleanVar <- function(data) {
    name.list <- names(data)
    name.list <- gsub("TEAM_", "", name.list)
    names(data) <- name.list
    data
}

# apply the function
train <- cleanVar(train)
test <- cleanVar(test)

# check data once again
str(train)
str(test)
```

```{r, include=F}
# Train.Mod being made here
train.mod <- subset(train, select = -c(BATTING_HBP))
train.mod <- as.data.table(train.mod)
dummies <- dummyVars(~ ., data = train.mod[, -1])
train.dummy <- predict(dummies, train.mod)
pre.process <- preProcess(train.dummy, method='bagImpute')
imputation <- as.data.frame(predict(pre.process, train.dummy))
```

# Data exploration

## Possible writeup for Data Exploration

Professionals and gamblers alike are always seeking to optimize their chances of winning, whether it be sports, games, or their bets on them. Major League Baseball is a [multibillion dollar industry](https://www.forbes.com/sites/mikeozanian/2018/04/11/baseball-team-values-2018/#4675cfd43fc0) where individual teams, players, and those who profit off of their success stand to benefit most from such optimization. 

In order to determine the best way to infer whether the 162 games in a baseball team's year will result in more wins overall, data from 1871 to 2006 where each set of values represented a season for an unnamed team, totalling 2,276 records. For each team their number of wins in a given year were given with a maximum possible of 162 wins, in addition to that team's base hits, doubles, triples, homeruns, walks, and strikeouts by batters, batters hit by pitches, bases stolen by batters and the number of times they were caught stealing, the number of errors, double plays, walks, hits, and homeruns allowed, and strikeouts by pitchers. 

```{r}
# We should probably rename the variables so that they're easier to read
# and don't require a separate file or scrolling to see what each stands for
as.data.frame(describe(train))
# train.desc <- describeBy(train)
# train.desc <- as.data.frame(train.desc)
# subset(train.desc, select = c("min", "mean", "median", "max", "sd"))
```

Of all the observations gathered across these fifteen variables, 10.187% were missing information; batters hit by pitches was missing the most, with 2,085 instances of missing information. The standard deviation of the various variables hints at the intense skewing of some of the variables provided, especially the hits allowed, walks allowed and strike outs.

```{r}
# I couldn't figure out how to remove the "Frequency" on the ylab.
plot_histogram(train, title="Baseball Features")
```

The theoretical effect of strikeouts by batters, batters caught stealing, errors, walks, hits, and homeruns allowed were believed to theoretically have a negative impact on the number of wins of an individual team in a given year. A closer look at the correlation between the variables painted a different picture.

```{r}
max_sd = 3 # change this number to change the threshold for how many standard deviations from the mean are acceptable
imputed_train <- cbind(train$TARGET_WINS, imputation)
names(imputed_train)[1] <- "TARGET_WINS"
outliers <- sapply(imputation, function(x) ifelse(x < mean(x)+(sd(x)*max_sd), TRUE, NA))
#outliers <- sapply(imputed_train[,-1], function(x) ifelse(findInterval(x, c(mean(x)-(sd(x)*max_sd),mean(x)+(sd(x)*max_sd)), rightmost.closed = T) == 1, TRUE, NA))
imputed_train = imputed_train[complete.cases(outliers),]
corr.train <- round(cor(imputed_train),3)
ggcorrplot::ggcorrplot(corr.train, 
                       type = 'upper',
                       lab=T,
                       lab_size=2,
                       title="Correlation")
```

When compared to what was hypothesized, there was actually a positive impact for the number of wins for a team in a given year by walks, hits, and homeruns allowed; at the same time, variables previously thought to have a positive correlation - strikeouts by pitchers and double plays - had a negative correlation for the number of wins. The three variables with the greatest correlation to the number of wins were the hits allowed, the walks by batters, and the walks allowed. Of these, the hits allowed had a relatively low correlation with the walks by batters and the walks allowed, whereas the walks allowed and the walks by batters had a direct positive correlation with one another.

* Describe the size: 

The money ball data is 144kb in size. The data contains 2,276 rows and 16 columns without the index. The variables are continuous integer. The `TARGET_WINS` is our response variable. There are 3,478 missing values out of 36,416 observations.

* Statistics summary
```{r}
describe(train)
```

* Data visualization
```{r}
# Histograms
plot_histogram(train)
```

```{r}
melt.train <- melt(train)
# Boxplot
ggplot(melt.train, 
       aes(factor(variable), value)) + 
  geom_boxplot(aes(variable,value)) + 
  scale_y_log10() + 
  coord_flip() +
  labs(title="Boxplot",
       x="", 
       y="log transformed freq.")
```

```{r}
melt.to.wins <- melt(train, id.vars=c('TARGET_WINS'))
# Scatterplot
ggplot(melt.to.wins, 
       aes(x=value, y=TARGET_WINS)) + 
  geom_point() + 
  facet_wrap(~variable, scale = "free") + 
  geom_smooth(model="lm") + 
  labs(title="Scatterplot",
       x="", 
       y="Number of Wins")
```

```{r}
# Correlations
corr.train <-round(cor(train),3)
ggcorrplot::ggcorrplot(corr.train, 
                       type = 'upper',
                       lab=T,
                       lab_size=2,
                       title="Correrlation")
```


```{r}
# Missing values
#table(is.na(train)) #3478 missing values
#sapply(train, function(x) sum(is.na(x)))
plot_missing(train)
```

# Data preparation

## Possible writeup for Data Preparation

As previously mentioned, just north of 10% of the data was missing values. Missing values can lead to errors in a model, bias, and worse if left unaccounted for. Attempting to "fix" this by imputing values or guessing why the values are missing in the first place - such as concluding that the missing values are meant to be zeroes - are just as likely to help with creating a model as it is to help with creating a disaster.

One of the R packages utilized, DataExplorer, recommends removing null or missing values; it was for this reason all observations of hits by pitch were removed.<< SOURCE FOR THIS IS REQUIRED! IF NO SOURCE IS PROVIDED, CONSIDER USING "Due to the sheer volume of missing values present in the observations for hits by pitch (91.61%) it was determined the best course of action was to remove the variable altogether." OR A VARIATION THEREOF. >> Deleting all cases with missing values, in this instance, would have shrunk the size of the dataset down to less than a tenth of its original size. For this reason, the feature itself was excluded from the dataset, rather than the cases that had no values present for it.

The other missing values - present in batting strikeouts... needs more work. x_x

## Information in general, don't use the writeups here as-is

## Missing Values

1) `Hit by pitch` missing 91.61% . 

* Missing values can lead to errors and bias into a model. Fixing and imputation may help or make it worse.
* When it is just a few observations missing, modifications can be made, however, with 91.61% is a large proportion and could distort the modelling later on that it is better to ignore this column.
- The Data explorer package recommends to remove.
- From LMR: Missing Completely at Random (MCAR) The probability that a value is missing is the same for all cases. If we simply delete all cases with missing values from the analysis, we will cause no bias, although we may lose some information.
* However, there is no consensus on when to exclude missing data. Some argue that missing data more than 10% can lead to bias. Others argue that missing data patterns have greater impact than the proportion.


```{r eval=F}
# Fix missing values
# remove BATTING_HBP
train.mod <- subset(train, select = -c(BATTING_HBP))
```

2) `Pitching_SO` and `Batting_SO` are missing exact same proportion 4.48% and are missing in the same observations.

```{r eval=F}
train.mod <- as.data.table(train.mod)
#train.mod[BATTING_SO == 0] == train.mod[PITCHING_SO == 0]
```

## NA Imputation

```{r eval=F}
# preProcess can be used to impute data sets based only on information in the training set
# see reference: http://topepo.github.io/caret/pre-processing.html

dummies <- dummyVars(~ ., data = train.mod[, -1])
train.dummy <- predict(dummies, train.mod)

pre.process <- preProcess(train.dummy, method='bagImpute')
imputation <- as.data.frame(predict(pre.process, train.dummy))
```

```{r}
sapply(train.mod, function(x) sum(is.na(x)))

train.mod[, `:=`(BATTING_SO = imputation$BATTING_SO,
          BASERUN_SB = imputation$BASERUN_SB,
          BASERUN_CS = imputation$BASERUN_CS,
          PITCHING_SO = imputation$PITCHING_SO,
          FIELDING_DP = imputation$FIELDING_DP)]
```


```{r}
par(mfrow=c(4,2))
hist(train$BASERUN_SB)
hist(train.mod$BASERUN_SB)

hist(train$BASERUN_CS)
hist(train.mod$BASERUN_CS)

hist(train$PITCHING_SO)
hist(train.mod$PITCHING_SO)

hist(train$FIELDING_DP)
hist(train.mod$FIELDING_DP)
```

## Feature Engineering

```{r}
imputed_train$PB_H <- imputed_train$PITCHING_H - imputed_train$BATTING_H
imputed_train$PB_HR <- imputed_train$PITCHING_HR - imputed_train$BATTING_HR
imputed_train$PB_BB <- imputed_train$PITCHING_BB - imputed_train$BATTING_BB
imputed_train$PB_SO <- imputed_train$PITCHING_SO - imputed_train$BATTING_SO
```

----

FOR THE OTHER HALF OF THE GROUP:

z_train <- sapply(imputed_train, scale)
log_train <- log(imputed_train) # weird results
z_log_train <- sapply(log_train, scale) # weirder results

imputed_train is most likely the variable you want to use.
